---
title: "Practical Time Series Analysis"
output: html_notebook
---

In a (weak) stationary time series, there is no:

* systematic change in mean (no trend)
* systematic change in variance
* periodic variations

**Autocovariance**

$$
\gamma_k = \gamma(t,t+k) \approx \frac{\sum^{N-k}_{t=1}(x_t - \bar{x})(x_{t+k}-\bar{x})}{N}
$$

**Autocorrelation**

$$
\rho_k = \frac{\gamma_k}{\gamma_0} \\
\rho_k = \frac{\sum^{N-k}_{t=1}(x_t - \bar{x})(x_{t+k}-\bar{x})}{\sum^N_{t=1}(x_t - \bar{x})^2}
$$

## Random Walk

### Brownian Motion

```{r}
random_process = cumsum(rnorm(100))
plot(random_process)
```
```{r}
acf(random_process, main='')
```
the above is clearly non-stationary process. We can get a stationary process in this case by subtracting the previous value from current value:

$$
X_t - X_{t-1} = N(0,1)
$$

```{r}
acf(diff(random_process), main='')
```

### Moving average process

$$
X_t = Z_t + w_1 Z_{t_z}+w_2 Z_{t-2}+\dots+w_qZ_{t-q}
$$

notice the difference from the Brownian motion:

$$
X_t = \sum_{i=1}^t Z_i
$$

```{r}
T = 100
noise = rnorm(T)
ma_2 = NULL

for (i in 3:T) {
  ma_2[i-2] = noise[i] + .7*noise[i-1] + .2*noise[i-2]
}

par(mfrow=c(2,1))

plot(ma_2, type='l', ylab='', col='blue')
acf(ma_2, main='')
```
## Stationarity

We say a process is weakly stationary if:

1. Mean function: $\mu(t) = \mu$
2. ACF: $\gamma(t_1, t_2) = \gamma(t_2 - t_1) = \gamma(\tau)$

The point 2 means that it doesn't matter where you are in the process, but how far they are separated. **Not sure what this means**. 

Question: why stationarity is crucial in formulating a model from data?

### Problem 1

Given a MA(2) process: 

$X_t = Z_t + .5Z_{t-1} + .5Z_{t-2}, \sigma^2=1$.

What is the autocovariance at lag zero? That is, calculate $\gamma(0)$?

**Solution**

$$
\gamma(0) = \frac{\sum^{N-k}(x_t - \bar{x})(x_{t+k}+\bar{x})}{N} \\
= E[X_tX_t]
= E[X_t^2]
$$
$$
E[X_t^2] = E[(\beta_0Z_t + \beta_1Z_{t-1} + \beta_2Z_{t-2})^2] \\
E[\beta_0^2Z_t^2] = \beta_0^2\sigma \\
E[\beta_1^2Z_{t-1}^2] = \beta_1^2\sigma^2 \\
E[\beta_2^2Z_{t-2}^2] = \beta_2^2\sigma^2 \\
$$

$E[\beta_i\beta_jZ_{t-i}Z_{t-k}] = 0$, where $i \neq j$, since $Z_{t-i}$ and $Z_{t-k}$ are independent their expected value is 0. Therefore $E[X_t^2]$:

$$
\gamma(0) = \sigma^2 \sum_{i=0}^{k} \beta_i^2 \\
= \sigma^2(1^2 + .5^2 + .5^2) = 1.5
$$

### Problem 2

Given the same MA(2) process as specified above, calculate the autocorrelation function at lag 2 (k=2).

$$
E[X_tX_{t+k}] = E[(\beta_0Z_t + \beta_1Z_{t-1}+\beta_2Z_{t-2})(\beta_0Z_{t+k} + \beta_1Z_{t+k-1}+\beta_2Z_{t+k-2})] \\
= E[\beta_0\beta_0Z_tZ_{t+k}+\beta_0\beta_1Z_tZ_{t+k-1} + \beta_0\beta_2Z_tZ_{t+k-2}+\\
\beta_1\beta_0Z_{t-1}Z_{t+k}+\beta_1\beta_1Z_{t-1}Z_{t+k-1} + \beta_1\beta_2Z_{t-1}Z_{t+k-2}+\\
\beta_2\beta_0Z_{t-2}Z_{t+k}+\beta_2\beta_1Z_{t-2}Z_{t+k-1} + \beta_2\beta_2Z_{t-2}Z_{t+k-2}
]
$$
If you examine all the terms inside the above expectation they are all 0 except for those that $k \leq q$ (where k is lag and q is the order of the term), thus in our case only terms $Z_t$ and $Z_{t+k-2}$ (k = 2, q = 2, k=q) are non-zero:
$$
E[\beta_0\beta_0Z_tZ_{t+k}] = 0 \\
E[\beta_1\beta_0Z_tZ_{t+k-1}] = 0 \\
E[\beta_0\beta_2Z_tZ_{t+k-2}] = \beta_0\beta_2E[Z_tZ_{t+k-2}] = \beta_0\beta_2E[Z_t^2] = \beta_0\beta_2\sigma^2 \\
$$

therefore $\gamma(2) = \beta_0\beta_1\sigma^2$, given $\sigma^2=1, \beta_0=1, \beta_2=.5$, $\gamma(2) = .5$.

We are not done here, we still need to compute $\rho(2) = \frac{\gamma(2)}{\gamma(0)} = .5/1.5 = .033$